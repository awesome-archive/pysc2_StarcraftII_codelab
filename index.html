<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, minimum-scale=1, initial-scale=1, user-scalable=yes">
    <!-- REMOVE THIS METATAG on public launch! -->
    <meta name="robots" content="noindex">

    <title>Starcraft II Learning Environment</title>
    <meta name="description" content="train-your-own-agent-with-pysc2">

    <!-- See https://goo.gl/OOhYW5 -->
    <link rel="manifest" href="/manifest.json">

    <script src="/bower_components/webcomponentsjs/webcomponents-lite.js"></script>

    <link rel="import" href="/bower_components/codelab-components/google-codelab-elements.html">


    <link rel="stylesheet"
          href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
    <style>
      body {
        font-family: "Roboto",sans-serif;
        background-color: #F8F9FA;
      }
      paper-button.colored {
        background-color: #068c43;
    </style>
</head>
<body>

<google-codelab
        title="Train your first Starcraft II learning agent"
        environment="web"
        feedback-link="https://github.com/codelab_pysc2/weblink?/issues"
        last-updated="2016-12-01">

       
    <!-- STEP 1 -->


    <google-codelab-step
            label="Presentation"
            step="1"
            duration="5">
        <p>

            <img src="pysc2_images/pysc2_0_header.jpg" style="max-width: 750px" alt="Starcraft II Learning environment">
        <h2>Overview</h2>
        <p>
            In this Codelab, you will learn about StarCraft II Learning Environment project and to train your first Deep
            Reinforcement Learning agent. You will also get familiar some of the concepts and frameworks to get to train
            a machine learning agent.</p>
        <aside class="special">
            <p>
                <strong>Note:</strong> The goal is to offer a general overview of how the StarCraft II Learning
                environment works,
                bring out some machine learning concepts to get started.
            </p>
        </aside>

        </p>
        <h2>Why StarCraft II ? </h2>
        <p>Computer games provide a compelling solution to the issue of evaluating and comparing different learning ,
            control and planning approaches on standard tasks. They present themselves as perfect sandboxes to agents to
            be trained before being applied to real world problems .
            Behind these challenges we´re facing a General Purpose Learning problem, as those agents are systems that
            are capable of learning how to adapt in the real world .</p>

        <p>StarCraft II - SC2 - is a Real Strategy Game developed by Blizzard after StarCraft I and BloodWar. It is an
            adversarial game : the goal is to collect resources,
            build a base, build some units and defeat the opponent that is trying to do the same thing.</p>
        </p>
        <p class="checklist">StarCraft II is a suitable challenge for machine learning as it presents as a game : </p>
        <ul>
            <li><strong>Vast and diverse action space</strong>. There are listed approximately 500 actions as options
                that can be taken
                by an agent , and actions available that are displayed by the API . Actions among a combinatorial space
                of approximately 10<sup>e8</sup> possibilities , divided in unit movements , building types and the set
                of actions evolves as the game and player does .
                The basic set of actions are approximately 50 times more than Atari games, so it adds a significant
                scale jump to the choices an agent might take ,
                which leads to think about hierarchical actions, in both macro -strategy and economy- and micro -melee
                management-.
            </li>

            <li><strong>Imperfect information game.</strong> Full game is a partially observable Markov decision process
                for the agent - due to Fog of War configuration-which forces the agent to explore in order to control de
                map and guess what the opponent is doing.
            </li>
            <li><strong>Multi-agent</strong> problem in high and low level: as several players compete for influence and
                resources and each player controls hundred of units which need to collaborate to achieve the goal.
            </li>
            <li><strong>Sequential problem</strong>. The player must take early decisions with consequences that may not
                be seen until much later in the game.
                This situation makes the challenge suitable with Reinforcement Learning credit assignment problem.
            </li>
        </ul>


        <h2 class="checklist">What you'll learn</h2>
        <ul>
            <li>What pySC2 and StarCraft II API are</li>
            <li>Basics theoretical concepts of Deep Reinforcement Learning to get started</li>
            <li>Basic interaction with StarCraft II API</li>
            <li>Training and modify Machine Learning agent</li>
            <li>Evaluating how training and evaluating works</li>
            <li>Train a Machine Learning agent in GCP</li>


        </ul>

        <h2 class="checklist">What you´ll need</h2>
        <ul>
            <li>A computer connected to the internet with Python3 installed</li>
            <li>The library pySC2 installed</li>
            <li>Starcraft II game -no license required-</li>
            <li>keras</li>
            <li>keras-rl</li>
        </ul>

        <h2 class="checklist">What you need to know</h2>
        <ul class="checklist">
            <li>Basic driving of bash console</li>
            <li>Basic knowledge of Python - data structures, lists, functions, loops -</li>
            <li>Basic knowledge of object-oriented programming -classes, methods, decorators -</li>
            <li>Conceptual knowledge about Neural Networks</li>
        </ul>
        <aside class="special">
            <p>
                <strong>Note:</strong>The Colab install requirements takes a considerable amount of time, so the
                suggestion is to get ahead in the theory while installation is running
            </p>
        </aside>
        <google-codelab-survey survey-id="test-survey-123"><h4>How would rate your experience with Machine
            Learning?</h4>


            <paper-radio-group>
                <paper-radio-button>Novice</paper-radio-button>
                <paper-radio-button>Intermediate</paper-radio-button>
                <paper-radio-button>Advanced</paper-radio-button>
            </paper-radio-group>
        </google-codelab-survey>
    
        <h3>About The Author</h3>
        <ul>
            <li>Gema Parreño <a href="https://github.com/SoyGema" target="_blank" rel="noopener">Github</a></li>
        </ul>    
    
        <p>
            <small><i>"With the right training, anyone can be in two places at once.
                But their mind must be fully open. Oh, and they need a psionic amplifier. That helps too."</i>
                Repeatedly selected Protoss unit quote
            </small>
        </p>




       
    </google-codelab-step>
         
            
            
    <!-- STEP 2 -->


    <google-codelab-step
            label="Lab : Installing Requirements"
            step="2"
            duration="10">
        <p>

            <img src="pysc2_images/pysc2_1_header.jpg" style="max-width: 750px" alt="Install requirements">
        <p>
            <small><i>"The stars align"</i> Void Raid Protoss move order unit quote</small>
        </p>

        <aside class="special">
            <p>
                <strong>Note:</strong> This installation is linked to a local machine. If you want to
                run the experiment in the cloud ( recommended for scalability ) you can go to step 9
            </p>
        </aside>


        <h2>Installation of pySC2</h2>
        <p>This install section has been taken from <a href="https://github.com/deepmind/pysc2" target="_blank"
                                                       rel="noopener">PySC2</a> documentation.
            You can install it using pip : </p>
        <pre><code>
 $ pip install pysc2      # Python 2.7
 $ pip3 install pysc2     # Python 3.6
      </code></pre>
        <p>Or with git</p>
        <pre><code>
 $ git clone https://github.com/deepmind/pysc2.git       # Clone repository
 $ pip install pysc2/                                    # Install it with pip or pip3
      </code></pre>

        <h2>Install the Game</h2>
        <p>Install StarCraft II -note that you don´t need to have the game paid- of SC2 for developing Machine learning.
            PySC2 expects the game to be in <code>Applications/StarCraftII/</code>&#160;.
            For Linux, Follow Blizzard´s <a href="https://github.com/Blizzard/s2client-proto#downloads" target="_blank"
                                            rel="noopener">here</a> documentation and click on the Linux Package
            versions be sure to install versions from 3.16.1 to above .
            Be sure to type the password <strong>‘iagreetotheeula’</strong> linked to the machine learning license . For
            Windows and Mac , Install the game as normal from <a href="https://us.battle.net/account/download/"
                                                                 target="_blank" rel="noopener">Battle.net</a></p>

        <h2>Download mini-games and full ladder maps</h2>
        <p>Download the <a href="https://github.com/Blizzard/s2client-proto#downloads" target="_blank" rel="noopener">ladder
            maps</a> and the <a href="https://github.com/SoyGema/pysc2_StarcraftII_codelab/blob/master/mini_games.zip"
                                target="_blank" rel="noopener">mini-games</a> and put them into your <code>StarcraftII/Maps/</code>&#160;
            folder. If the folder doesn´t exist , create it.
        </p>

        <h2>Download the Codelab material</h2>
        <p>This Codelab will launch an agent from a mini-game made by some community members during 2017 BlizzCon AI
            workshop .
            So be sure to download it</p>
        <p><a href="https://github.com/SoyGema/pysc2_StarcraftII_codelab/blob/master/pysc2_codelab_material.zip"
              target="_blank">
            <paper-button class="colored" raised>
                <iron-icon icon="file-download"></iron-icon>
                Download source code
            </paper-button>
        </a></p>
        <p class="checklist">Inside the <code>.zip</code>&#160; Codelab material file, you might find</p>
        <ul class="checklist">
            <li><strong>HallucinIce.SC2Map:</strong> SC2 mini-game developed during BlizzCon 2017 AI workshop.</li>
            <li><strong>agent_CNN+LSTM.py:</strong> python agent that we will train in this CodeLab.</li>
            <li><strong>scripted_agent.py:</strong> python Symbolic AI agent developed during BlizzCon 2017 AI workshop.
            </li>
            <li><strong>requirements.txt:</strong> requirements for the CodeLab.</li>
        </ul>
        <h2>Install requirements</h2>
        <p class="checklist"> Go inside the <code>pysc2_codelab_material</code>&#160; folder and type :</p>
        <pre><code>
 $   pip install requirements.txt
      </code></pre>
        <ul>
            <li><p>Place the <code>scripted_agent.py</code>&#160; in <code>Applications/StarCraftII/pysc2/agents</code>&#160;</p></li>
            <li><p>Place the <code>agent_CNN+LSTM.py</code>&#160; in <code>Applications/StarCraftII/pysc2/agents</code>&#160;</p></li>

            <h2>Configure mini-game</h2>
        </ul>
        <p class="checklist">Once there, follow the instructions : </p>
        <ul>
            <li>Go to <code>site-packages\pysc2\maps\mini_games.py</code>&#160; and add to mini-games array the following mini-games
                names
            </li>


            <pre><code>
mini_games = [  ## This mini-games names should already been in your list
    "BuildMarines",  # 900s
    "CollectMineralsAndGas",  # 420s
    "CollectMineralShards",  # 120s
    "DefeatRoaches",  # 120s
    "DefeatZerglingsAndBanelings",  # 120s
    "FindAndDefeatZerglings",  # 180s
    "MoveToBeacon",  # 120s   ##Now you add this few lines
    "SentryDefense", # 120s
    "HallucinIce", # 30s <--- NEW NAME
]
      </code></pre>

            <li>Place HallucinIce.SC2Map in <code>Applications/StarCraftII/Maps/mini_games</code>&#160;</li>
            <li>Test the environment.
                In your console, you can type the mini-game map name
            </li>
            <pre><code>
 $ python -m pysc2.bin.agent --map HallucinIce
      </code></pre>
            <p>Once you type this in your console, a small screen simulating a game should appear,
                together within a feature layer screen . A random agent is now playing the game, taking
                random actions .</p>
        </ul>
        <aside class="special">

            <h2>Congratulations!</h2>
            <p>Now you have set up this challenging environment for machine learning research</p>
        </aside>

        <p>
            <small>"The merging is complete" Trained Adept Protoss unit quote</small>
        </p>
    </google-codelab-step>


    <!-- STEP 3 -->


    <google-codelab-step
            label="Theory : Deep Reinforcement Learning Overview"
            step="3"
            duration="15">
        <p>

            <img src="pysc2_images/pysc2_2_header.jpg" style="max-width: 750px"
                 alt="Theory Deep Reinforcement Learning">
        <p>
            <small><i>"Prismatic core online"</i> Stalker Protoss repeatedly selected order unit quote</small>
        </p>

        <h2>The Reinforcement Learning Problem</h2>

        <p>The paradigm of learning by trial-and-error, exclusively from rewards is known as Reinforcement Learning .
            The essence of RL is learning through interaction , mimicking the human way of learning<sup>[1]</sup> with
            an interaction with environment and has its roots in behaviourist psychology .</p>

        <p>For a definition of the Reinforcement Learning problem we need to define an environment in which a <strong>perception-action-learning
            loop</strong><sup>[fig1]</sup> takes place .
            In this environment, the agent observes a given state t
            The agent, leaning in the policy, interacts with the environment by taking an action in a given state that
            may have long term consequences .
            It goes into a next state with a given timestep t<sub>+1</sub> and updates the policy.
            At the end, the agent receives observations/states from the environment and a reward as a sign of feedback,
            and interacts with the environment through the actions. </p>

        <center><img src="pysc2_images/pysc2_RL_perception_action_learning.gif" style="max-width: 450px"
                     alt="Learning loop"></center>
        <center>
            <p>
                <small>[fig1] Perception-action-learning loop</small>
            </p>
        </center>

        <p>In RL , we trait both prediction and control problems, linked respectively to state-value or action-value function.
            When you go down algorithmically in this challenge, it comes in how you treat the value function and how the
            policy strategy affects to the agent .</p>
        The Reinforcement Learning problem can be described formally into a Markov Decision Process or MDP: it describes
        an environment for Reinforcement Learning,
        the surroundings or conditions in which the agent learns or operates.
        The Markov process is a sequence of states with the Markov property ,
        which claims that the future is independent of the past given the present .
        That makes that we only shall need the last state to evaluate the agent.</p>

        <aside class="special">
            <p>
                <strong>Note:</strong>The goal of RL is to find an optimal policy which achieves the maximum expected return from all states.

            </p>
        </aside>

        <h2>RL Agents</h2>
        <p class="checklist">Reinforcement Learning agents may include one or more of these components <sup>[2]</sup>.
        </p>
        <ul>
            <li><strong>Value function</strong> V(s). Acts as an evaluator and process how much reward we expects to get
                , or, in other words, measures how well we are doing .
                In RL taxonomy, you can find a classification that includes value based or policy based agents.
            </li>
            <li><strong>Policy</strong>: Π . Understood as the Agent´s behaviour function. The policy maps from state to
                action , and can be deterministic or stochastic .
                RL taxonomy includes <strong>ON policy algorithms</strong> -learning on the job- or <strong>OFF
                    policy</strong> algorithms - learning from other behaviour-
                Policy ON and OFF algorithms differentiate about how it calculates the Value Function .
                On-policy algorithms have no memory and V(s) comes from the same policy. In Policy-off algorithms , V(s)
                come from another policy .
            </li>
            <li><strong>Model</strong>. Agent´s representation of the environment .
                RL taxonomy establish model free and model based agents. Model-based have the knowledge about the
                environment and can be partial or totally observable .
            </li>
        </ul>

        <p>There are two main approaches to solving RL problems : Value-function and Policy Search<sup>[fig2]</sup> .
            There is an hybrid explored during last year, also known as Actor-Critic approach </p>
        <center><img src="pysc2_images/pysc2_RL_policy.gif" style="max-width: 450px" alt="Learning loop"></center>
        <center>
            <p>
                <small>[fig2] Policy learning explanation graphic approach</small>
            </p>
        </center>

        <p>How Policy learning works : In a state S<sub>t</sub>, we compute Q(s,a) and it takes one of the actions that
            helps to achieve the goal .
            The agents acts with the environment by executing actions, and the environment acts with the agent by giving
            it the observations inherited from the actions the agent took.</p>
        <h2>The value function </h2>
        <br> </br>
        <center><img src="pysc2_images/pysc2_value_function.jpg" style="max-width: 450px" alt="Learning loop"></center>
        <p class="checklist">The Value function is a mathematical abstraction that acts as an evaluator of the agent.
            The Reinforcement Learning taxonomy and creation approach might define itself on how we treat the value
            function.
            It is important to underline that there are two kinds of value function :</p>
        <ul>
            <li><strong>State-value function</strong>. V(s) : measures how good is to be in a particular state s.</li>
            <li><strong>Action-value function</strong>. Q (s, a) : measures How good is to take a particular action .
                Bellman Optimality equation, which is basic for defining the RL solution approach , is based in the
                Action-value function.
            </li>
        </ul>

        <p>In Dynamic Programming context, policy search do not need to maintain a value function model, but search
            for an optimal policy .The process of producing an agent comes in two phases : policy evaluation and policy
            iteration ( improvement ) . They treat prediction and control problem respectively.<sup>[2]</sup>
            In some cases innovations come in the way we think and threat the value function.</p>


        <h2>What is Deep Reinforcement Learning?</h2>
        <p>Reinforcement Learning is an area of Machine Learning based on mimicking the human way of
            learning<sup>[1]</sup>. Deep learning is enabling reinforcement learning to scale decision-making to problems
            that were previously intractable, specially those with settings with high-dimensional state and action spaces.
            We will see later on that agents from pySC2 construct and learn their own knowledge directly from raw inputs
            , such as vision, using Deep Learning.
            The combination of these approaches, with the goal of achieving human-level performance across many
            challenging domains receive the name of Deep Reinforcement Learning<sup>[3]</sup>.
            DRL can deal with some of the problems that DL has like the curse of dimensionality<sup>[6]</sup>.
            One of the driving forces behind DRL is the vision of creating systems that are capable of learning how to
            adapt in the real world. It seems likely that in the future, DRL will be an important component in constructing
            general AI systems.</p>

        <aside class="special">
            <p>
                <strong>Note</strong> DQN = Deep Neural Network for Q Learning <strong>approximation</strong>
            </p>
        </aside>

        <p><sup>[4]</sup>In general, DRL is based on training deep neural networks to approximate the optimal policy Π, and/or the
            optimal value functions V(s), Q and A .
            However, deep Q-networks are one way to solve deep RL problem . Another state of the art algorithm , A3C,
            introduces methods based on asynchronous RL.</p>
        <p>Video games may be an interesting challenge, but learning how to play them is not the end goal of DRL <sup>[4]</sup>,
            as the main goal is the vision of creating systems that are capable of learning how to adapt in the real
            world .
            In this Codelab, we dig into a DRL agent for getting started with the machine learning challenge. Having
            said that, the environment present itself suitable for Reinforcement Learning in general and not only DRL
            . </p>


        <h3>DQN Algorithm <strong>Overview</strong><sup>[fig3]</sup></h3>

        <p>In a high level, the algorithm takes the input as the RGB pixels and gives the output of the value of the
            suitable actions that the agent might take with a given policy .
            It initializes random action-value functions Q (s, a) and in a loop of all episodes and all timesteps,
            follow a greedy policy in which selects a random action or the one in which the action-value function is
            higher with probability ε .
            Execute action and store transition tuple in memory .</p>

        <center><img src="pysc2_images/DQN_algorithm.jpg" style="max-width: 750px" alt="DRL_pseudoCode"></center>
        <center>
            <p>
                <small>[fig3] Deep Q-Network(DQN), adapted from Mnih et al(2015)</small>
            </p>
        </center>


        <h3>DQN Algorithm <strong>optimization</strong> : Target Network , Experience Replay and Hubber Loss</h3>

        <p><strong>Experience Replay</strong> allows training using stored memories from its experience, and avoids to
            forget states that the Neural model hasn´t seen in a while.
            The idea behind experience replay consists in : at each Q-learning iteration, you play one step in the game,
            but instead of updating the model based
            on that step, you add the information from the step you took to a memory and then make a batch sample of
            that memory for training<sup>[3]</sup>.

            The <strong>Target Network</strong> brings stability to learning. Every a number of iterations, we will
            make a copy of the Q-Network and use it to compute the target instead of the current Q-network.

            <strong>Hubber loss</strong> comes to stabilize learning in short and long term, as this function os
            quadratic for small values and linear for large values.
            For those familiarized with the loss function coming from Neural Networks , it comes to say that it diverges
            into two different implementations.</p>

        <h3>DQN <strong>variations</strong>: Double Deep Q-learning with dueling Network Architecture</h3>

        <><strong>Double Q-learning</strong></strong><sup>[fig4]</sup> is a model-free policy off algorithm can be used at scale to
            successfully reduce the overoptimism inherited from DQN, resulting in more stable and reliable learning.
            In Double Deep Q learning, two value functions <strong>V(s)</strong> are calculated : one set of weights is
            used to determine the greedy policy - <strong>improvement</strong>- and the other is used to determine its
            value -<strong>evaluation</strong>.
            In this case, the evaluation network trains and from time to time it passes the weights to the improvement
            network. Then the improvement network learns and passes it again to the evaluation network<sup>[5]</sup>.</p>

        <center><img src="pysc2_images/Double_DQN.png" style="max-width: 750px" alt=""></center>
        <center>
            <p>
                <small>[fig4] Dueling DQN algorithm</small>
            </p>
        </center>

        <p><strong>Dueling DQN</strong> helps to generalize learning and the key insight behind the architecture is that
            for many states, it is unnecessary to estimate the value of each action choice. </p>
        Besides, it decouples the idea of action-value <strong>Q(s,a)</strong> function into state-value function
        <strong>V(s)</strong> and advantage function <strong>A(s,a)</strong>, which leads to better policy evaluation,
        and has shown good results <sup>[7]</sup>, shining in attention and focus.</p>
        <p>The dueling network has two streams to separately estimate scalar state-value and the advantages for each
            action: a module implements an equation - non trivial- to combine them.</p>

        <aside class="special">
            <p>
                <strong>Note</strong> We use DQN architecture and Double Deep Q learning with dueling architecture for a reason : the key ideas for this architecture are reduce overoptimism and help with
                generalization.
            </p>
        </aside>

        <aside class="special">
            <h2>Congratulations!</h2>
            <p>Now you have passed though the agent and the main concepts of Machine Learning needed for the Codelab.
                <p>Thanks for being around!</p>
        </aside>


    </google-codelab-step>


    <!-- STEP 4 -->


    <google-codelab-step
            label="Theory : pySC2 environment"
            step="4"
            duration="15">
        <p>

            <img src="pysc2_images/pysc2_3_header.jpg" style="max-width: 750px" alt="Theory pysc2 environment">
        <p>
            <small><i>(Sound effect)</i> Probe Protoss selected order unit quote</small>
        </p>

        <h2>Overview</h2>

        <p>Find in this section a ripped version of pySC2 research published <a href="https://arxiv.org/abs/1708.04782"
                                                                                target="_blank" rel="noopener">paper</a>
            by DeepMind and Blizzard.</p>
        <p>StarCraft II presents itself as a grand challenge for Reinforcement Learning for several reasons : the
            imperfect information of the game while playing forces to guess what the opponent is doing -curiously , it
            is Partially Observable Markov Decision Process due to Fog of War-.
            Besides, the huge action space -combinatorial space of 10<sup>e8</sup> possibilities - urges the need for
            <strong>hierarchy management</strong>, that might change as the tech player strategy evolves .
            The economy management mixed with melee management urges the agent to solve problems at different scales.
            The real-time simultaneousness, with fast paced decisions and multitasking management presents the credit
            assignment problem as a necessity to be solved .</p>
        <p>As the paper underlines, StarCraft II Learning environment offers a new environment for exploring DRL
            algorithms and architectures.
            Here we will explore and ripped some useful concepts about the environment that might be use to understand
            them .
            The full potential of this environment has yet to be seen , as it is a multi-agent environment at high and
            low level : several players compete for control over the map and resources , and ultimately win the match .
            In the low level, each player controls an army that must cooperate to achieve a common goal .</p>

        <p class="checklist">StarCraft II learning environment release consists of 3 subcomponents, 2 of those cited
            here : </p>
        <ul>
            <li><a href="https://github.com/Blizzard/s2client-proto" target="_blank" rel="noopener">SC II API</a> . Used
                for start a game, get observations, take actions and review replays .
            </li>
            <li><a href="https://github.com/deepmind/pysc2" target="_blank" rel="noopener">pySC2</a> Using SC II API ,
                pySC2 is a Python environment that wraps the StarCraft II API to ease the interaction between agents and
                StarCraft II .
                It defines action and observation specification.
            </li>
        </ul>

        <h2>Game description and reward structure</h2>

        <p>The ultimate goal of the game is to win, so is the reward structure defined as win(+1) / tie(0) / loss(-1) .
            The ternary win/tie/loss is sparse enough to be accompanied by Blizzard score, which gives an intuition
            about the player´s development during the game - training units, building buildings and researching tech -
            and might, in some cases, be used as a Reinforcement Learning reward.
            In order to win, the player should : accumulate resources, construct production buildings , amass an army
            and eliminate all of the opponent´s building. Other strategies might be used to win the game </p>


        <h2>Actions and observations</h2>

        <p><strong>Observations</strong></p>

        <p>The games exposes observation from <strong>RGB pixels</strong> exposed as rgb_screen and rgb_minimap - which
            configure view as a human would see - and in a <strong>Feature layers</strong> structure . Extra information
            beyond the screen is exposed as rgb_minimap Feature, like the pySC2 interpreter .
            Inside features we must differentiate in between minimap feature layers and screen feature layers. One is
            looking at the screen and the other is looking at the top bottom left minimap . </p>
        <p>Besides that, it offers as a structured information 10 Tensors with different data with relevant information
            for the player.</p>

        <center><img src="pysc2_images/pysc2_feature_layers.png" style="max-width: 624px" alt="Actions"></center>
        <center>
            <p>
                <small>Feature Layer view, initialized with the game</small>
            </p>
        </center>


        <p><strong>Actions</strong></p>

        <p>Each action has its own argument or lists of arguments and respond for being choices that the agent can make.
            Action functions have been created to solve this problem: these are availabe at pysc2.lib.actions base code.
            The actions are the output of the agent in every state t, that means, the decisions the agent must take in
            every time-step …
            PySC2 agent calls display available actions.</p>

        <center><img src="pysc2_images/pysc2_action.gif" style="max-width: 624px" alt="Actions"></center>
        <center>
            <p>
                <small>Action Space atomic level defined in pySC2 - Image by Deepmind -</small>
            </p>
        </center>

        <h2>Agent Neural Net architectures</h2>
        <p>In DQN , The Neural Networks act as an approximation function to find the Q-values.
            In pySC2 research, DeepMind has tried several Neural Networks architectures , which include the Atari and
            CNN+LSTM<sup>[fig5]</sup> architectures.
            Both of them processes <strong>spatial</strong> -screen- and <strong>non-spatial</strong> -vector of
            features- .The difference among the two of them is that Atari combines the Spatial and non-spatial features
            into a flatten layer that process the agent output, unrolling it into a space action policy through
            coordinates ( x and y ) and a value that will take the action .
            In the Neural Net architecture proposed by pySC2 paper it combines the spatial and non-spatial features into
            a NN .</p>

        <center><img src="pysc2_images/pysc2_FullConvarchitecture.gif" style="max-width: 450px"
                     alt="NeuralNet Architecture"></center>
        <center>
            <p>
                <small>[fig5] Full Conv Architecture</small>
            </p>
        </center>

        <h2>What are mini-games?</h2>

        <p>Mini-games are bounded challenges of the game worth of be investigated in isolation.
            The purpose of those is to test a subset of actions or game mechanics with a clear reward structure .
            This maps must be configured as Markov Decision Processes , with a initial setup, timing and reward
            structure that encourages the agent to learn a desired behaviour and to avoid local optima.
            Reward structure might have the same structure as the full game ( win/tie/loss) . This mini-games are
            characterized by restricted action sets, custom reward functions and/or time limits.
            If you want to see more mini-games you can visit <a href="https://github.com/SoyGema/pySC2_minigames"
                                                                target="_blank" rel="noopener">the community mini-games
                repository</a> or build your own <a
                    href="https://github.com/SoyGema/Startcraft_pysc2_minigames/tree/master/docs" target="_blank"
                    rel="noopener">tutorial</a>
            , transforming DefeatRoaches mini-game into a designed melee in your DefeatWhatever configuration.</p>

        <p>Tackling the whole SC2 problem is really hard, so mini-games comes to solve a smaller problem.
            Some thoughts may led you to think that mini-games are only useful if the knowledge is portable of a larger
            AI game
            , but it really depends on your goal. There is still room for great mini-games to come up with interesting
            nested problems for both micro and macro in SC2.<sup>[8]</sup></p>

        <h2>Results</h2>

        <p>Here we have the comparison over different mini-games from pySC2 paper , in which there is an overview of how
            you can use different Neural Network Architectures .</p>
        In the Y axis, we find the score of the game, in the X axis we find the training time .
        As we can see, FullyConv and Atari Net start giving best results than the FullyConv with memory in the case of
        Defeat Roaches mini-game.
        However, this paradigm changes in the long term and so does the performance of different architectures in
        different challenges.
        The graph shown above illustrates how the long tail shows that the early performance seems to be better in
        certain kind of NN architectures but at
        the end it changes and shows the best performance on LSTM+CNN . In Defeat Roaches case, starting learning
        quicker doesn´t mean that there will be a better
        performance when the NN stabilizes itself . </p>

        <center><img src="pysc2_images/pysc2_results.png" style="max-width: 624px"
                     alt="Results of performance in different mini-games"></center>


    </google-codelab-step>


    <!-- STEP 5 -->


    <google-codelab-step
            label="Lab : Quickstart overview of pySC2 "
            step="5"
            duration="10">
        <p>

            <img src="pysc2_images/pysc2_4_header.jpg" style="max-width: 750px" alt="Quickstart overview of pySC2">
        <p>
            <small><i>"The task ahead is difficult, but worthwhile"</i> Karax Protoss repeatedly selected unit quote
            </small>
        </p>


        <h2>Configuration</h2>
        <p>You can configure all the requirements for getting started in the <a href="https://github.com/deepmind/pysc2"
                                                                                target="_blank" rel="noopener">pysc2</a>
            official
            repository or in the step 2 or step 9 of this Codelab .
            In this section we will test the running environment with the mini-game that we are going to train.</p>
        <h2>HallucinIce mini-game</h2>
        <p><strong>Description</strong></p>
        <p>The mini-game is an imbalanced melee in between Terran and Protoss. The goal is to learn to exploit sentry
            Hallucination function. The desired behaviour is to make Terran believe they´re fighting a much larger
            force than anticipated, and ultimately, to make Protoss units win.</p>


        <p class="checklist"><strong>Initial State</strong></p>
        <ul>
            <li>4 Sentry at left playable size</li>
            <li>4 Hellions at right playable size</li>
            <li>2 Reapers at right playable size</li>
        </ul>

        <p><strong>Reward system</strong></p>
        <p>Protoss defeated : -10 Terran defeated : +10</p>

        <p><strong>End conditions</strong></p>
        <p>Time elapsed or Zerg defeated</p>

        <h2>Run Random Agent</h2>
        <p>Open a terminal , go to <code>Applications/StarCraftII/pysc2</code>&#160; and type</p>
        <pre><code>
$ python -m pysc2.bin.agent --map HallucinIce
      </code></pre>

        <p>You should see something like this</p>

        <center><img src="pysc2_images/pysc2_HallucinIceRandom.gif" style="max-width: 524px" alt="Random Hallucination">
        </center>


        <p>As you can check, there is nor learning neither strategy on these episodes, as the agent is selecting random
            actions.
            Let´s see how a random agent works </p>

        <pre><code>
class RandomAgent(base_agent.BaseAgent):
  """A random agent for StarCraft II pysc2."""

  def step(self, obs):
    super(RandomAgent, self).step(obs)
    function_id = numpy.random.choice(obs.observation.available_actions)
    args = [[numpy.random.randint(0, size) for size in arg.sizes]
            for arg in self.action_spec.functions[function_id].args]
    return actions.FunctionCall(function_id, args)
      </code></pre>


        <p>Basically, the RandomAgent class extends from the <code>base_agent</code>&#160; and we define what is doing at each step
            ,in which defines a variable that selects randomly an available action and call it with its arguments at
            each step</p>

        <h2>Run Scripted Agent</h2>
        <p>Scripted agents are a set of handcrafted rules that help the agents take a strategy
            You can type in your console</p>
        <pre><code>
$ python -m pysc2.bin.agent --map HallucinIce  --agent pysc2.agents.scripted_agent.HallucinIce
      </code></pre>


        <p>And should see something like this</p>

        <center><img src="pysc2_images/pysc2_HallucinIceArchon.gif" style="max-width: 500px" alt="Archon Hallucination">
        </center>

        <pre><code>
class HallucinationArchon(base_agent.BaseAgent):
  """An agent specifically for solving the HallucinIce map with Archon Unit for StarCraft II pysc2."""

  def step(self, obs):
    super(HallucinationArchon, self).step(obs)
    if _HAL_ARCHON in obs.observation["available_actions"]:
      player_relative = obs.observation.feature_screen.player_relative
      hellion_y, hellion_x = (player_relative == _PLAYER_HOSTILE).nonzero()
      if not hellion_y.any():
        return FUNCTIONS.no_op()
      index = numpy.argmax(hellion_y)
      target = [hellion_x[index], hellion_y[index]]
      return FUNCTIONS.Hallucination_Archon_quick("now", target)
    elif _SELECT_ARMY in obs.observation["available_actions"]:
      return FUNCTIONS.select_army("select")
    else:
      return FUNCTIONS.no_op()
      </code></pre>

        <p>This agent creates a class in which observes if the Archon Hallucination is possible ,
            locates the position of the enemy and calls the selection and Hallucination action.</p>

        <p>Scripted agents might be able to execute programmed orders fast and straightforward.
            However, these agents are narrow ,
            not able to execute orders outside those rules and, in the last phase, don´t learn and generalize well .
            In the next step we will jump into the machine learning agent </p>

        <aside class="special">
            <h2>Hey!</h2>
            <p>After this step you might be able to code any scripted agent and produce
                a random one for the mini-games and execute it
                as a base for benchmarking with your machine learning agent.</p>
        </aside>


    </google-codelab-step>


    <!-- STEP 6 -->


    <google-codelab-step
            label="Lab : Jumping into the machine learning agent "
            step="6"
            duration="20">
        <p>
            <img src="pysc2_images/pysc2_5_header.jpg" style="max-width: 750px" alt="Quickstart overview of pySC2">
        <p>
            <small><i>"Willingly."</i> Mothership Protoss selected order unit quote</small>
        </p>


        <h2>Into the machine learning agent</h2>


        <p class="checklist"> Here is presented an overview of the agent´s code : an informal walk-through and
            descriptive inspection of the code, in order to help others to understand and improve the implementation .
            In a functional overview, the code is making the following steps.</p>

        <ul>
            <li>Import statements from libraries : pySC2, <a href="https://keras.io" target="_blank" rel="noopener">keras</a>
                and <a href="https://keras-rl.readthedocs.io/en/latest/" target="_blank" rel="noopener">keras-rl</a>
            </li>
            <li>Load actions from the API</li>
            <li>Configure flags and parameters</li>
            <li>Configure processor with observations and batches</li>
            <li>Define the environment</li>
            <li>Agent model DNN Architecture</li>
            <li>Process of training the game</li>

        </ul>

        <h3>Agent Overview and DNN Architecture</h3>

        <p> We understand the agent as the Learner , the decision maker .
            There might be many different agents that can be used for this challenge.
            The goal of the agent is to learn a policy -control strategy- that maximizes the expected return
            -cumulative, discounted reward-.
            The agent uses knowledge of state transitions, of the form (s<sub>t</sub>, a<sub>t</sub>, s<sub>t+1</sub>, r<sub>t+1</sub>)
            in order to learn and improve its policy .
            In DQN , we use a Neural Network as a function approximator for the Q-values.</p>
        Deep Q Learning is a model-free off policy algorithm that will define the agent.
        If you want to know more about the basis of the agent , jump into The Step 3 and read what is Deep Reinforcement
        learning.</p>


        <p>The agent is constructed looking at the feature layers with a CNN-LSTM network.
            This network is an LSTM architecture specifically designed for sequence prediction
            problems with inputs, like images or videos. This architecture involves using Convolutional Neural
            Networks(CNN)
            layers for feature extraction on input data combined with LSTMS to support
            sequence prediction .</p>

        <p>In a general approach, this neural net architecture has been used for Activity Recognition,
            Image and video description<sup>[9]</sup> </p>

        <aside class="special">
            <p>
                <strong>Note:</strong> We are implementing Double Deep Q Learning with dueling architecture,
                A variation from DQN with the main idea of reduce overoptimism and help with generalization.

            </p>
        </aside>

        <pre><code>
 def neural_network_model(input, actions):
    model = Sequential()
    # Define CNN model
    print(input)
    model.add(Conv2D(64, kernel_size=(5, 5), input_shape=input))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
    model.add(Dropout(0.3))

    model.add(Conv2D(128, kernel_size=(3, 3), input_shape=input))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
    model.add(Dropout(0.3))

    model.add(Conv2D(256, kernel_size=(3, 3)))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
    model.add(Dropout(0.3))
    model.add(Flatten())

    model.add(Dense(256, activation='relu'))
    model.add(Reshape((1, 256)))
    # Add some memory
    model.add(LSTM(256))
    model.add(Dense(actions, activation='softmax'))
    model.summary()
    model.compile(loss="categorical_crossentropy",
                  optimizer="adam",
                  metrics=["accuracy"])

    return model
      </code></pre>

        The Neural Network architecture is made of 3 conv2D layers of different kernel_sizes with dropout , and add it
        a last layer of LSTM .

        <img src="pysc2_images/pysc2_Neural_Net_Graph.jpg" style="max-width: 750px" alt="Quickstart overview of pySC2">


        <h3>Policy and Agent</h3>
        <pre><code>

    # Policy
    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr="eps", value_max=1, value_min=0.2, value_test=.0,
                                  nb_steps=1e6)
      </code></pre>

        <p>The policy helps with the selection of action to take on an environment.
            The linear annealing policy computes a current threshold value and transfers it
            to an inner policy which chooses the action. The threshold value is following a linear
            function decreasing over time. In this case we use <code>EpsGreedyQPolicy</code>&#160; action selection, which
            means that a random action is selected with probability eps.The <code>value_max</code>&#160; and <code>value_min</code>&#160;
            threshold settled regulates how the agent explores the environment and then gradually sticks to
            what it knows<sup>[10]</sup>.
        </p>
        <pre><code>
    # Agent
    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, enable_double_dqn=True,
                   enable_dueling_network=True,
                   nb_steps_warmup=500, target_model_update=1e-2, policy=policy,
                   batch_size=150,
                   processor=processor,
                   delta_clip=1)

    dqn.compile(Adam(lr=.001), metrics=["mae", "acc"])

      </code></pre>

        <p>The keras-rl DQNAgent class that calls the agent
            The <code>model</code>&#160; refers to the Neural Network coded above , so if you change the model, you can
            have different neural networks as an approximation function,
            the <code>nb_actions</code>&#160; take the actions available for the agent, that
            are printed when you run the agent in the console.
            The <code>target_model_update</code>&#160; and <code>delta_clip</code>&#160; parameters
            related to optimization and stable learning of Deep Reinforcement learning: target model update will tell us
            how oftenly the weights will be transferred to the memory. Delta_clip parameter refers
            to Huber loss implementation. </p>

        <p>Another callbacks like Tensorboard are added in order to visualize the Graph and useful metrics.</p>

        <p>DQN Algorithm has its own challenges . In keras-rl library you can implement in a
            straightforward way Replay memory, target Network and Huber loss by hyperparameters.
            For further evolutions and variations of DQN, you can study and implement
            Rainbow algorithm</p>

        </p>
        <aside class="special">
            <h2>Congratulations!</h2>
            <p>Now you have passed though the agent and the main concepts of DRL </p>
            <p>This Codelab step had a huge conceptual load!</p>
        </aside>


    </google-codelab-step>


    <!-- STEP 7 -->


    <google-codelab-step
            label="Lab : Running and training the agent"
            step="7"
            duration="15">
        <p>

            <img src="pysc2_images/pysc2_6_header.jpg" style="max-width: 750px" alt="Quickstart overview of pySC2">
        <p>
            <small><i>"The Void will answer"</i> Mohandar Protoss trained order unit quote</small>
        </p>


        <h3>Running and training</h3>

        <p>It´s time to get the agent running!
            Type in your console .</p>

        <pre><code>
$ python3 CNN_LSTM.py
    </code></pre>


        <h3>Visualizing in Tensorboard</h3>

        <p>There exist a pre-configured callback in the agent that allows
            to you to run Tensorboard. Once you start your training, type in your console.
            You should see something like this. Note that the path/Graph will be created once the training has
            started.</p>

        <pre><code>
$ tensorboard --logdir path/Graph --host localhost --port 8088
    </code></pre>

    </google-codelab-step>


    <!-- STEP 8 -->


    <google-codelab-step
            label="Lab : Customize the  enviroment and agent"
            step="8"
            duration="20">
        <p>

            <img src="pysc2_images/pysc2_7_header.jpg" style="max-width: 750px" alt="Quickstart overview of pySC2">
        <p>
            <small><i>"Hierach?"</i> Talandar Protoss selected order unit quote</small>
        </p>


        <h3>Try your own mini-game in your machine</h3>

        <p class="checklist">You can try your own mini-game by changing the following steps in your pySC2 folder</p>
        <ul>
            <li><strong>Create your mini-game</strong>. You can visit this <a
                    href="https://github.com/SoyGema/Startcraft_pysc2_minigames/tree/master/docs" target="_blank"
                    rel="noopener">tutorial</a> that helps you transforming DefeatRoaches into 'DefeatWhatever'
                melee</strong>
            <li><strong>Add</strong> your mini-game to the array in <code>pysc2\maps\mini_games.py</code>&#160;</li>
            <li><strong>Add</strong> your mini-game <code>.SC2</code>&#160; file to the folder <code>StarcraftII\maps\mini_games</code>&#160;
            </li>
            <li><strong>Inside</strong> the agent file, add the mini-game name into the Flag config and training_game
                section of the <code>agent_CNN+LSTM.py</code>&#160;
            </li>
            <li>Run the agent in your console</li>
        </ul>

        If you want to test it against already built-in mini-games from DeepMind and Blizzard :

        <ul class="checklist">
            <li>Add the mini-game name into the Flag config of the <code>agent_CNN+LSTM.py</code>&#160;</li>
            <li>Run the agent in your console</li>
        </ul>
        <pre><code>
$ python -m pysc2.bin.agent --map DefeatWhatever
      </code></pre>

        <h3>Your own policy or your own RL agent</h3>

        <p>You can customize the agent provided in several ways :
            You can change the policy and still use a Double Deep Q Learning with dueling architecture agent, that
            means that you will take another approach to the learning
            strategy but still use a Neural Network for function approximation.

            The trade-off between exploration and exploitation is challenging and an on-going
            research topic. A recommended approach by keras-rl authors is Boltzmann-style exploration<sup>[10]</sup>, so if you
            would like,
            give it a try and feel free to share your results !

        </p>


        <h3>Try pre-trained A3C in your own mini-game</h3>

        <p> In xhujoy <a href="https://github.com/xhujoy/pysc2-agents" target="_blank" rel="noopener">repository</a>
            ,this member of the community -- have code A2C and A3C and have generously released a pre-trained agent that
            you can download <a href="https://drive.google.com/file/d/0B6TLO16TqWxpUjRsWWdsSEU3dFE/view" target="_blank"
                                rel="noopener">here</a>

            You can test your own mini-game, but this is what happens when HallucinIce mini-game was tested with A3C:</p>

        <center><img src="pysc2_images/pysc2_HallucinICEA3C.gif" style="max-width: 300px"
                     alt="Quickstart overview of pySC2"></center>

        <p>As you can check, the agent learnt to hide and do nothing. Which take to think that not all algorithms are
            suitable
            for all the goals.


        </p>

    </google-codelab-step>


    <!-- STEP 9 -->


    <google-codelab-step
            label="Lab : Train in GCP with a VM instance"
            step="9"
            duration="30">
        <p>

            <img src="pysc2_images/pysc2_9_header.jpg" style="max-width: 750px" alt="Train in google cloud">
        <p>
            <small><i>"Is it that simple?"</i> Kerrigan on Kaldir</small>
        </p>


        <h3>Enable Cloud configuration</h3>
        <center><img src="pysc2_images/pysc2_GCP_Compute_Engine.jpg" style="max-width: 150px"
                     alt="Train in google cloud"></center>

        <p>We will mainly use the Compute Engine module in the Google Cloud Platform, which offers Scalable virtual
            machines for training the agent in the cloud.
            .You can run the experiment in the cloud using instances from Gcloud commands or the platform.
        <p> An <strong>instance</strong> is a virtual machine (VM) hosted on Google´ infrastructure. </strong>

        <h3 class="checklist">Initial Config</h3>

        <h4><strong>1</strong>.Create a GCP project </strong>Go to the <a href="https://console.cloud.google.com/"
                                                                          target="_blank" rel="noopener">google cloud
            platform console</a>
            <h4><strong>2</strong>.Enable billing</h4>
            <h4><strong>3</strong>. Create a project </h4>
            <h4><strong>4</strong>. Create a VM Instance </h4>
            <p>In the left menu, Go to <strong>Compute Engine</strong> and from there, click in the VM Instances page
                and <strong>Create an instance</strong></p>
            <center><img src="pysc2_images/pysc2_VM_instance.jpg" style="max-width: 250px" alt="Train in google cloud">
            </center>
            <p>For this experiment we will work with a Ubuntu 16.04 LTS with 8 CPUs and 52GB of memory </p>
            <center><img src="pysc2_images/pysc2_VM_instance2.jpg" style="max-width: 450px" alt="Train in google cloud">
            </center>
            <p>Once the instance is created, go to the left menu and select <strong>Disks</strong> options and edit the
                instance.
                As we might need some extra space due to the requirements of installing the game, edit the size and add
                20GB extra space .</p>

            <aside class="special">
                <p>
                    <strong>Note:</strong> You can also solve this steps of Compute Engine using gcloud commands
                </p>
            </aside>

            <p>Once you are there, open the VM instance terminal and follow the following code instructions:</p>
            <h3>StarCraft II Learning environment in the Cloud</h3>
            <p>Download StarCraft II </p>
            <pre><code>
$ !wget https://blzdistsc2-a.akamaihd.net/Linux/SC2.4.0.2.zip
$ !unzip -P iagreetotheeula -oq SC2.4.0.2.zip -d ~
    </code></pre>

            <p>Download mini-games </p>
            <pre><code>
$ !wget –quiet https://github.com/deepmind/pysc2/releases/download/v1.0/mini_games.zip
$ !unzip -P iagreetotheeula -oq mini_games.zip -d ~/StarCraftII/Maps/
    </code></pre>

            <p>Install requirements and agent </p>
            <p>Download the .zip material from this Codelab </p>
            <pre><code>
$ !wget https://github.com/SoyGema/pysc2_StarcraftII_codelab/blob/master/pysc2_codelab_material.zip
$ !unzip pysc2_codelab_material.zip
    </code></pre>
            <p>Install Codelab requirements </p>
            <pre><code>
$ cd pysc2_codelab_material
$ pip install requirements.txt
    </code></pre>
            <p>This should take a while but actually install the libraries needed for the experiment</p>


            <p>Run the agent </p>
            <p>Go inside the folder and execute the agent in a <strong>nohup</strong> command, that is best suited for
                long job runs. </p>
            <pre><code>
$ nohup python agent_CNN+LSTM.py > agent_CNN+LSTM.nohup &
    </code></pre>
            <p>This command should put the agent to train. A Graph folder might appear.</p>
            <p>If you want to visualize how your training is going on, you can write on console</p>
            <pre><code>
$ more agent_CNN+LSTM.nohup
    </code></pre>

            <p>For Tensorboard visualization</p>
            <p>Configure a VPC Network within the GCP Platform, adding it to tcp:6006 port</p>
            <pre><code>
$ tensorboard --logdir Graph
    </code></pre>


            <p>If you want to give it a try to train PySC2 agents using <a href="https://medium.com/@paul.steven.conyngham/how-to-get-blizzard-google-deepminds-pysc2-working-for-free-on-colabs-be2e68f18893" target="_blank" rel="noopener">Colaboratory</a> tool, you can go to this
                developed during Nvidia workshop in April 2018</p>
            </ul>

        <br> </br>
        <br> </br>
    </google-codelab-step>


    <!-- STEP 9 -->


    <google-codelab-step
            label="Congrats! Bibliography , resources and more"
            step="9"
            duration="5">
        <p>

            <img src="pysc2_images/pysc2_8_header.jpg" style="max-width: 750px" alt="Quickstart overview of pySC2">
        <p>
            <small><i>"What is it you seek, my friend?"</i> Fenix Protoss repeatedly selected unit quote</small>
        </p>


        </p>

        <h3>You´ve made it!</h3>
        <p>It´ been an interesting ride. Now you know more about the StarCraft II Learning environment.</p>


        <h3>More resources</h3>

        <p class="checklist">You can visit different sites with variety of depths.</p>
        <ul>
            <li><a href="http://us.battle.net/sc2/en/blog/20944009/the-starcraft-ii-api-has-arrived-8-9-2017"
                   target="_blank" rel="noopener">Blizzard´s</a> and <a
                    href="https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/"
                    target="_blank" rel="noopener">DeepMind´s</a> blogposts
            </li>
            <li> SCII API sc2client-proto <a href="https://github.com/Blizzard/s2client-proto" target="_blank" rel="noopener">repository</a>
            </li>
            <li>PySC2 <a href="https://github.com/deepmind/pysc2" target="_blank" rel="noopener">repository</a> and
                paper
            </li>
            <li>PySC2 selected <a href="https://itnext.io/build-a-zerg-bot-with-pysc2-2-0-295375d2f58e" target="_blank"
                                  rel="noopener">tutorials</a> by Steven Brown
            </li>
            <li>RL <a href="http://chris-chris.ai/2017/08/30/pysc2-tutorial1/" target="_blank"
                      rel="noopener">tutorials</a> by Cris-cris
            </li>
            <li>Community <a href="https://github.com/SoyGema/pySC2_minigames" target="_blank"
                             rel="noopener">mini-games</a> repo
            </li>

        </ul>

        <h3>Bibliography</h3>

        <p class="checklist">The following articles, papers, blog articles and conversations were used to make this
            codelab possible
        <ul>
            <li>[1] DeepMind  <a href="https://deepmind.com/blog/deep-reinforcement-learning/" target="_blank" rel="noopener">blog article</a> Deep Reinforcement Learning</li>
            <li>[2] <a href="" target="_blank" rel="noopener">David Silver´s RL</a> Course </li>
            <li>[3] Human-level control through Deep Reinforcement Learning <a href="https://www.nature.com/articles/nature14236" target="_blank" rel="noopener">Nature Paper </a>. </li>
            <li>[4] A Brief <a href="https://arxiv.org/pdf/1708.05866.pdf" target="_blank" rel="noopener">Survey</a> of Deep Reinforcement Learning</li>
            <li>[5] Deep Reinforcement Learning with Double Q-learning<a href="https://arxiv.org/abs/1509.06461" target="_blank" rel="noopener">paper</a> </li>
            <li>[6] keras-rl Github policy</li>
            <li>[7] Dueling Network Architectures for Deep Reinforcement Learning <a href="http://proceedings.mlr.press/v48/wangf16.pdf" target="_blank" rel="noopener">paper</a></li>
            <li>[8] SC2 AI Discord conversation during September 2018</li>
            <li>[9] CNN+LSTM <a href="https://www.quora.com/How-does-the-CNN-LSTM-model-work" target="_blank" rel="noopener">Quora</a> </li>
            <li>[10] keras-rl documentation</li>
        </ul>

        <h3>About the Author</h3>
        <div class="box">
    <img src="pysc2_images/pysc2_codelab_author_Gema.jpeg"  style="max-width: 50px">
    <span style="">Gema Parreño.</span>
    <span style=""><a href="https://github.com/SoyGema" target="_blank" rel="noopener">Github</a> and <a href="https://twitter.com/SoyGema" target="_blank" rel="noopener">Twitter</a>.</span>
        </div>

        <br> </br>
        <br> </br>



    </google-codelab-step>
